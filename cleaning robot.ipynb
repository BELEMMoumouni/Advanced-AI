{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This  exercise uses the mdp tool box developped at INRAE\n",
    "#    https://pymdptoolbox.readthedocs.io/en/latest/index.html\n",
    "        \n",
    "#Install it (if not already installed) and import the toolbox and the examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mdptoolbox, mdptoolbox.example, mdptoolbox.util\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# The Cleaning Robot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dead line : 11 nov for both groups (note book + report describing your mdp, including a drawing)\n",
    "\n",
    "Consider a house-cleaning robot. It can be either in the living room or at its charging station - or out  of battery. \n",
    "The living room can be clean or dirty. So there are five states: LD(in the living room, dirty), LC(in the living room, clean), CD(at the charger, dirty), CC(at the charger, clean), O (out of power).\n",
    "\n",
    "\n",
    "####  \n",
    "When in the living room,    the robot  can either choose to clean or to charge. \n",
    "\n",
    "When in the charging station, the robot can either choose to clean or to keep charging.\n",
    "\n",
    "When out of order, any of the two actions (clean, charge) leads to the same results: staying out of power\n",
    "\n",
    "####  \n",
    "\n",
    "The reward for being  in a clean state is rc \n",
    "\n",
    "The reward for being in a dirty state is rd < rc at each time step\n",
    "\n",
    "The reward for being out of power is  costcrash  -  lower or equal to rd ; let us set it equal to rd  (the living rooms becomes dirty anyway)\n",
    " \n",
    "\n",
    "####  \n",
    "\n",
    "When  the robot decides to  clean,\n",
    "*  if it is in the living room, then it may become out of power with proba e\n",
    "*  if it is in the charging station, no risk of running out of power   \n",
    "*  cleaning a clean floor leaves it clean\n",
    "*  cleaning a dirty floor can sometimes fail (even is battery is ok) - let eps be the probability of fail of the cleaning\n",
    "     \n",
    "When  the robot decides to  charge,  action charge always takes the robot to the charging station. The  dirtiness of the room can go from clean to dirty with a probability  pDust  and it stays for sure at the dirty level if dirty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Goal\n",
    "\n",
    "Model the problem by a (infinite horizon) MDP (describe your model in details) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matrice_transions_reward(rc, rd, costcrash, e, eps, pDust):\n",
    "    R =np.array(\n",
    "        [[rd, 1000],\n",
    "        [rc, rc],\n",
    "        [rd, rd],\n",
    "        [rc, rc],\n",
    "        [costcrash, costcrash]])\n",
    "\n",
    "        #CLEAN\n",
    "       \n",
    "    P = np.array(\n",
    "         #LD  #LC  #CD  #CC  #O\n",
    "        [[[eps,1-eps-e,0.,0.,e], #LD\n",
    "        [0,1-e,0.,0.,e],         #LC\n",
    "        [eps,1-eps,0.,0.,0.],    #CD\n",
    "        [0.,1,0.,0.,0.],         #CC\n",
    "        [0.,0.,0.,0.,1]],       #O\n",
    "        \n",
    "        #CHARGE\n",
    "         #LD  #LC  #CD  #CC  #O\n",
    "        [[ 0., 0., 0., 1, 0.],       #LD\n",
    "         [ 0., 0., 0., 0., 1],       #LC\n",
    "         [ 0., 0., 1, 0., 0.],       #CD\n",
    "         [ 0., 0., pDust,1-pDust,0.],#CC\n",
    "         [ 0., 0., 0., 0., 1]]      #O\n",
    "        ])    \n",
    "        \n",
    "        \n",
    "    return P, R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rd= 5\n",
    "rc =10\n",
    "costcrash = 10\n",
    "e = 0.9\n",
    "eps = 0.1\n",
    "pDust = 0.1\n",
    "\n",
    "P, R = matrice_transions_reward(rc, rd, costcrash, e, eps, pDust)\n",
    "\n",
    "#finiteHorizon = mdptoolbox.mdp.PolicyIteration(P, R, 0.9)\n",
    "finiteHorizon = mdptoolbox.mdp.FiniteHorizon(P,R,0.9,1)\n",
    "finiteHorizon.run()\n",
    "finiteHorizon.setVerbose()\n",
    "finiteHorizon.policy[:,0]\n",
    "\n",
    "#LD LC CD CC O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the best policy.\n",
    "\n",
    "* When the probability of runing out of battery is high\n",
    "<br>For e = 0.9 , we got this policy: (1, 1, 0, 0, 0) and for e = 0.999999999999999999, we got (1, 0, 0, 0, 0) </br>\n",
    "* When it is low\n",
    "    For e = 0.1, we got (1, 1, 0, 0, 0)\n",
    "\n",
    "(explain the results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What policy is choosen if  the robot has a very good battery (i.e. the probability of being out  of charge is very low) ? \n",
    "\n",
    "    When the robot has a very good battery ( I set e = 0.1), we got this policy : (1, 1, 0, 0, 0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* What policy is choosen if the  living room gets dirty   quickly  (i.e.  pdust increases) ? \n",
    "\n",
    "    When pDust increases ( pDust = 0.8 or from 0.6 to 1), we got this policy : (1, 1, 0, 1, 0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* When e=0.1 does the best policy depend of rc ? of pDust?  \n",
    "\n",
    "    When e = 0.1, tests show that policy depends on rc at some level of rc rising. From beginning of tests with Around \n",
    "\n",
    "\n",
    "* What if costcrash  is independent of rc ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Paste your python program here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "Optional: extend your program so as to take into consideration several levels of battery (high, medium, low, very low for instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batteries = [\"high\", \"medium\", \"low\", \"very low\"]\n",
    "for batterie in batteries:\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "ee33b22a4495e93d58c602f85d93855cfb507df08ca7f946296387088ba3047b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
